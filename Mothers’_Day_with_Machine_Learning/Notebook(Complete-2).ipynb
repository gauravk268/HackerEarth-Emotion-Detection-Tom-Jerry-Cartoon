{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Classification\n",
    "\n",
    "Dataset and Problem Url:<br>\n",
    "https://www.hackerearth.com/challenges/competitive/hackerearth-machine-learning-challenge-mothers-day/\n",
    "\n",
    "Used Code from:<br>\n",
    "https://github.com/gauravbansal98/Sentiment-classification-on-news-of-disaster-on-social-media-02-06-2018<br>\n",
    "https://www.oreilly.com/content/perform-sentiment-analysis-with-lstms-using-tensorflow/<br>\n",
    "https://www.curiousily.com/posts/sentiment-analysis-with-tensorflow-2-and-keras-using-python/<br>\n",
    "\n",
    "# Got very low accuracy on test set, hence not used for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present working directory:  /mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning \n",
      "\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/idsMatrix.npy\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/Notebook(Complete-1).ipynb\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/Notebook(Not_Complete-1).ipynb\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/Notebook(Not_Complete-2).ipynb\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/test_tweet.csv\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/train_tweet.csv\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/.ipynb_checkpoints/Notebook(Complete-1)-checkpoint.ipynb\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/.ipynb_checkpoints/Notebook(Not_Complete-1)-checkpoint.ipynb\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/.ipynb_checkpoints/Notebook(Not_Complete-2)-checkpoint.ipynb\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/clean_tweet.csv\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/idsMatrix.npy\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/Submission.csv\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/test.csv\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/test_tweet.csv\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/train.csv\n",
      "/mnt/8CFE7DA1FE7D846C/Practice/Git/Active/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/train_tweet.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm   # can be used for showing progress bar for any iteration\n",
    "\n",
    "import random \n",
    "import time\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "PATH = os.getcwd()\n",
    "print(\"Present working directory: \", PATH, \"\\n\")\n",
    "\n",
    "for dirname, _,filenames in os.walk(PATH):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       original_text  sentiment_class\n",
      "0  Happy #MothersDay to all you amazing mothers o...                0\n",
      "1  Happy Mothers Day Mum - I'm sorry I can't be t...                0\n",
      "2  Happy mothers day To all This doing a mothers ...               -1\n",
      "3  Happy mothers day to this beautiful woman...ro...                0\n",
      "4  Remembering the 3 most amazing ladies who made...               -1\n",
      "                                       original_text\n",
      "0  3. Yeah, I once cooked potatoes when I was 3 y...\n",
      "1  Happy Mother's Day to all the mums, step-mums,...\n",
      "2  I love the people from the UK, however, when I...\n",
      "3  Happy 81st Birthday Happy Mother’s Day to my m...\n",
      "4  Happy Mothers day to all those wonderful mothe...\n"
     ]
    }
   ],
   "source": [
    "#Loading CSV Training Data\n",
    "\n",
    "train_data = pd.read_csv('./dataset/train.csv')\n",
    "train_data = train_data[['original_text', 'sentiment_class']]\n",
    "print(train_data.head())\n",
    "\n",
    "#Loading CSV Test Data\n",
    "\n",
    "test_data = pd.read_csv(\"./dataset/test.csv\")\n",
    "test_data = test_data[['original_text']]\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy  mothersday to all you amazing mothers o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy mothers day mum   i'm sorry i can't be t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy mothers day to all this doing a mothers ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy mothers day to this beautiful woman   ro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  sentiment_class\n",
       "0  happy  mothersday to all you amazing mothers o...                0\n",
       "1  happy mothers day mum   i'm sorry i can't be t...                0\n",
       "2  happy mothers day to all this doing a mothers ...               -1\n",
       "3  happy mothers day to this beautiful woman   ro...                0\n",
       "4  remembering the 3 most amazing ladies who made...               -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardizing text in the tweet, removing all types of sybmbols from the tweets\n",
    "\n",
    "def standardize_tweets(tweet, text_field):\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'http','')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'!','')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'@S\\+','')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]',' ')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'@','at')\n",
    "    tweet[text_field] = tweet[text_field].str.lower()\n",
    "    return tweet\n",
    "   \n",
    "    \n",
    "train_data = standardize_tweets(train_data, \"original_text\")\n",
    "# train_data.to_csv(\"train_tweet.csv\")\n",
    "\n",
    "test_data = standardize_tweets(test_data, \"original_text\")\n",
    "# test_data.to_csv(\"test_tweet.csv\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of a sentence:  38.76259659969088\n"
     ]
    }
   ],
   "source": [
    "def plot_length_sentences():\n",
    "    file = train_data\n",
    "    tweets = file['original_text']\n",
    "    sentences_length = []\n",
    "    for line in tweets:\n",
    "        words = word_tokenize(line)\n",
    "        sentences_length.append(len(words))\n",
    "        \n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    plt.xlabel('Length of sentences')\n",
    "    plt.ylabel('Number of sentences')\n",
    "    plt.hist(sentences_length, range=(0, 80))\n",
    "    plt.show()\n",
    "    \n",
    "    return sentences_length\n",
    "\n",
    "sentences_length = plot_length_sentences()\n",
    "print('Average length of a sentence: ',sum(sentences_length)/len(train_data['original_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 original_text\n",
       "sentiment_class               \n",
       "-1                         769\n",
       " 0                        1701\n",
       " 1                         765"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('sentiment_class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on O'Reilly Tutorial\n",
    "https://www.oreilly.com/content/perform-sentiment-analysis-with-lstms-using-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399999it [00:01, 373099.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word Vectors!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wordVectors = np.load(\"../glove.6B.100d.npy\")\n",
    "print(\"Loaded the word list!\")\n",
    "\n",
    "with codecs.open(\"../glove.6B.100d.vocab\", 'r', 'utf-8') as f_in:\n",
    "    wordsList = [line.strip() for line in tqdm(f_in)]\n",
    "    \n",
    "print('Loaded the word Vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399999\n",
      "(399999, 100)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26688  ,  0.39632  ,  0.6169   , -0.77451  , -0.1039   ,\n",
       "        0.26697  ,  0.2788   ,  0.30992  ,  0.0054685, -0.085256 ,\n",
       "        0.73602  , -0.098432 ,  0.5479   , -0.030305 ,  0.33479  ,\n",
       "        0.14094  , -0.0070003,  0.32569  ,  0.22902  ,  0.46557  ,\n",
       "       -0.19531  ,  0.37491  , -0.7139   , -0.51775  ,  0.77039  ,\n",
       "        1.0881   , -0.66011  , -0.16234  ,  0.9119   ,  0.21046  ,\n",
       "        0.047494 ,  1.0019   ,  1.1133   ,  0.70094  , -0.08696  ,\n",
       "        0.47571  ,  0.1636   , -0.44469  ,  0.4469   , -0.93817  ,\n",
       "        0.013101 ,  0.085964 , -0.67456  ,  0.49662  , -0.037827 ,\n",
       "       -0.11038  , -0.28612  ,  0.074606 , -0.31527  , -0.093774 ,\n",
       "       -0.57069  ,  0.66865  ,  0.45307  , -0.34154  , -0.7166   ,\n",
       "       -0.75273  ,  0.075212 ,  0.57903  , -0.1191   , -0.11379  ,\n",
       "       -0.10026  ,  0.71341  , -1.1574   , -0.74026  ,  0.40452  ,\n",
       "        0.18023  ,  0.21449  ,  0.37638  ,  0.11239  , -0.53639  ,\n",
       "       -0.025092 ,  0.31886  , -0.25013  , -0.63283  , -0.011843 ,\n",
       "        1.377    ,  0.86013  ,  0.20476  , -0.36815  , -0.68874  ,\n",
       "        0.53512  , -0.46556  ,  0.27389  ,  0.4118   , -0.854    ,\n",
       "       -0.046288 ,  0.11304  , -0.27326  ,  0.15636  , -0.20334  ,\n",
       "        0.53586  ,  0.59784  ,  0.60469  ,  0.13735  ,  0.42232  ,\n",
       "       -0.61279  , -0.38486  ,  0.35842  , -0.48464  ,  0.30728  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helloIndex = wordsList.index('hello')\n",
    "wordVectors[helloIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(line.split()) for line in train_data['original_text']]\n",
    "\n",
    "maxSeqLength = max(lengths)\n",
    "print(maxSeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66,)\n",
      "[   40   803     0  1004    14  7445     4 13766     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "numDimensions = 100\n",
    "\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "# firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1750],\n",
       "       [400000],\n",
       "       [     3],\n",
       "       [    63],\n",
       "       [    80],\n",
       "       [  5771],\n",
       "       [  5833],\n",
       "       [    65],\n",
       "       [    62],\n",
       "       [    40],\n",
       "       [   345],\n",
       "       [400000],\n",
       "       [   604],\n",
       "       [    35],\n",
       "       [   133],\n",
       "       [   666],\n",
       "       [     3],\n",
       "       [   252],\n",
       "       [   391],\n",
       "       [  5833],\n",
       "       [   372],\n",
       "       [    33],\n",
       "       [400000],\n",
       "       [    12],\n",
       "       [    63],\n",
       "       [     2],\n",
       "       [    94],\n",
       "       [     3],\n",
       "       [    87],\n",
       "       [   101],\n",
       "       [    52],\n",
       "       [    85],\n",
       "       [     3],\n",
       "       [  1507],\n",
       "       [400000],\n",
       "       [    95],\n",
       "       [  4589],\n",
       "       [   234],\n",
       "       [     2],\n",
       "       [   161],\n",
       "       [   907],\n",
       "       [400000],\n",
       "       [ 42772],\n",
       "       [ 10359],\n",
       "       [ 10107],\n",
       "       [400000],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0],\n",
       "       [     0]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength, 1), dtype='int32')\n",
    "\n",
    "indexCounter = 0\n",
    "tweet = train_data['original_text'][0]\n",
    "\n",
    "split = tweet.split()\n",
    "for word in split:\n",
    "    try:\n",
    "        firstFile[indexCounter] = wordsList.index(word)\n",
    "    except ValueError:\n",
    "        firstFile[indexCounter] = 400000\n",
    "    indexCounter = indexCounter + 1\n",
    "\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3235/3235 [03:24<00:00, 15.79it/s]\n"
     ]
    }
   ],
   "source": [
    "numFiles = len(train_data['original_text'])\n",
    "# print(numFiles)\n",
    "\n",
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "tweetCounter = 0\n",
    "\n",
    "for tweet in tqdm(train_data['original_text']):\n",
    "    indexCounter = 0\n",
    "    words = tweet.split()\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            ids[tweetCounter][indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            ids[tweetCounter][indexCounter] = 399999\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter > maxSeqLength:\n",
    "            print(\"Err. %f more that maxSeqLength , breaking loop!\" %indexCounter)\n",
    "            break\n",
    "    tweetCounter = tweetCounter + 1\n",
    "    \n",
    "np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_traning = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on curiousily\n",
    "https://www.curiousily.com/posts/sentiment-analysis-with-tensorflow-2-and-keras-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "type_one_hot = OneHotEncoder(sparse=False).fit_transform(\n",
    "    train_data.sentiment_class.to_numpy().reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp_train, temp_test, y_train, y_test = train_test_split(\n",
    "    train_data.original_text, type_one_hot, test_size=0.1,\n",
    "    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2911/2911 [03:10<00:00, 15.28it/s]\n",
      "100%|██████████| 324/324 [00:23<00:00, 13.89it/s]\n"
     ]
    }
   ],
   "source": [
    "numFiles = len(train_data['original_text'])\n",
    "\n",
    "X_train = np.zeros((temp_train.shape[0], maxSeqLength), dtype='int32')\n",
    "tweetCounter = 0\n",
    "\n",
    "for tweet in tqdm(temp_train):\n",
    "    indexCounter = 0\n",
    "    words = tweet.split()\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            X_train[tweetCounter][indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            X_train[tweetCounter][indexCounter] = 399999\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter > maxSeqLength:\n",
    "            print(\"Err. %f more that maxSeqLength , breaking loop!\" %indexCounter)\n",
    "            break\n",
    "    tweetCounter = tweetCounter + 1\n",
    "    \n",
    "X_test = np.zeros((temp_test.shape[0], maxSeqLength), dtype='int32')\n",
    "tweetCounter = 0\n",
    "\n",
    "for tweet in tqdm(temp_test):\n",
    "    indexCounter = 0\n",
    "    words = tweet.split()\n",
    "    \n",
    "    for word in words:\n",
    "        try:\n",
    "            X_test[tweetCounter][indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            X_test[tweetCounter][indexCounter] = 399999\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter > maxSeqLength:\n",
    "            print(\"Err. %f more that maxSeqLength , breaking loop!\" %indexCounter)\n",
    "            break\n",
    "    tweetCounter = tweetCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2911, 66) (2911, 3)\n",
      "(324, 66) (324, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               17152     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 50,435\n",
      "Trainable params: 50,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=256, input_shape=(X_train.shape[1], ), activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.01),\n",
    "    tf.keras.layers.Dense(units=128,activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.01),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2619 samples, validate on 292 samples\n",
      "Epoch 1/20\n",
      "2619/2619 [==============================] - 5s 2ms/sample - loss: 1.3026 - accuracy: 0.5296 - val_loss: 1.0386 - val_accuracy: 0.5034\n",
      "Epoch 2/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.6199 - accuracy: 0.5292 - val_loss: 1.0365 - val_accuracy: 0.5034\n",
      "Epoch 3/20\n",
      "2619/2619 [==============================] - 5s 2ms/sample - loss: 1.4552 - accuracy: 0.5292 - val_loss: 1.0388 - val_accuracy: 0.5034\n",
      "Epoch 4/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0147 - accuracy: 0.5300 - val_loss: 1.0384 - val_accuracy: 0.5034\n",
      "Epoch 5/20\n",
      "2619/2619 [==============================] - 5s 2ms/sample - loss: 1.0547 - accuracy: 0.5300 - val_loss: 1.0384 - val_accuracy: 0.5034\n",
      "Epoch 6/20\n",
      "2619/2619 [==============================] - 5s 2ms/sample - loss: 1.5866 - accuracy: 0.5300 - val_loss: 1.0384 - val_accuracy: 0.5034\n",
      "Epoch 7/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0157 - accuracy: 0.5300 - val_loss: 1.0383 - val_accuracy: 0.5034\n",
      "Epoch 8/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0157 - accuracy: 0.5300 - val_loss: 1.0386 - val_accuracy: 0.5034\n",
      "Epoch 9/20\n",
      "2619/2619 [==============================] - 7s 3ms/sample - loss: 1.0424 - accuracy: 0.5300 - val_loss: 1.0384 - val_accuracy: 0.5034\n",
      "Epoch 10/20\n",
      "2619/2619 [==============================] - 7s 3ms/sample - loss: 1.3038 - accuracy: 0.5292 - val_loss: 1.0385 - val_accuracy: 0.5034\n",
      "Epoch 11/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0165 - accuracy: 0.5296 - val_loss: 1.0387 - val_accuracy: 0.5034\n",
      "Epoch 12/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0163 - accuracy: 0.5296 - val_loss: 1.0387 - val_accuracy: 0.5034\n",
      "Epoch 13/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0160 - accuracy: 0.5300 - val_loss: 1.0387 - val_accuracy: 0.5034\n",
      "Epoch 14/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0159 - accuracy: 0.5300 - val_loss: 1.0383 - val_accuracy: 0.5034\n",
      "Epoch 15/20\n",
      "2619/2619 [==============================] - 5s 2ms/sample - loss: 1.0160 - accuracy: 0.5300 - val_loss: 1.0379 - val_accuracy: 0.5034\n",
      "Epoch 16/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0365 - accuracy: 0.5300 - val_loss: 1.0383 - val_accuracy: 0.5034\n",
      "Epoch 17/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.4531 - accuracy: 0.5296 - val_loss: 4.3291 - val_accuracy: 0.5068\n",
      "Epoch 18/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0667 - accuracy: 0.5296 - val_loss: 1.0389 - val_accuracy: 0.5034\n",
      "Epoch 19/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0165 - accuracy: 0.5296 - val_loss: 1.0388 - val_accuracy: 0.5034\n",
      "Epoch 20/20\n",
      "2619/2619 [==============================] - 6s 2ms/sample - loss: 1.0165 - accuracy: 0.5296 - val_loss: 1.0383 - val_accuracy: 0.5034\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,epochs=20, batch_size=5, validation_split=0.1, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 0s 322us/sample - loss: 19.0853 - accuracy: 0.5185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.085279517703587, 0.5185185]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (66,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b6ead06af815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (66,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "tweets = test_data['original_text']\n",
    "model.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
