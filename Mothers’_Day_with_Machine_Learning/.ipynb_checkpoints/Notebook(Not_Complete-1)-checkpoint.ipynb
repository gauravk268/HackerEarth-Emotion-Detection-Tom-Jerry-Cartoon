{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/glove.6B.100d.txt\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/Mothers’_Day_with_Machine_Learning.ipynb\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/clean_tweet.csv\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/train.csv\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/train_tweet.csv\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/test_tweet.csv\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/test.csv\n",
      "/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/.ipynb_checkpoints/Mothers’_Day_with_Machine_Learning-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm   # can be used for showing progress bar for any iteration\n",
    "\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "\n",
    "for dirname, _,filenames in os.walk('/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing directory to current working directory\n",
    "\n",
    "DATASET = '/home/gaurav/Desktop/Machine_Learning_Projects/Mothers’_Day_with_Machine_Learning/dataset/'\n",
    "os.chdir(DATASET)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  sentiment_class\n",
       "0  Happy #MothersDay to all you amazing mothers o...                0\n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be t...                0\n",
       "2  Happy mothers day To all This doing a mothers ...               -1\n",
       "3  Happy mothers day to this beautiful woman...ro...                0\n",
       "4  Remembering the 3 most amazing ladies who made...               -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading CSV Training Data\n",
    "\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "train_data = train_data[['original_text', 'sentiment_class']]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3. Yeah, I once cooked potatoes when I was 3 y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Mother's Day to all the mums, step-mums,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the people from the UK, however, when I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy 81st Birthday Happy Mother’s Day to my m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy Mothers day to all those wonderful mothe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text\n",
       "0  3. Yeah, I once cooked potatoes when I was 3 y...\n",
       "1  Happy Mother's Day to all the mums, step-mums,...\n",
       "2  I love the people from the UK, however, when I...\n",
       "3  Happy 81st Birthday Happy Mother’s Day to my m...\n",
       "4  Happy Mothers day to all those wonderful mothe..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading CSV Test Data\n",
    "\n",
    "test_data = pd.read_csv(\"./test.csv\")\n",
    "test_data = test_data[['original_text']]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>For shuffling the training data and parting it for test and train data</h2>\n",
    "<p>\n",
    "from sklearn.utils import shuffle<br>\n",
    "from sklearn.cross_validation import train_test_split<br>\n",
    "\n",
    "X, Y = shuffle(X,Y)<br>\n",
    "x_train = [ ]<br>\n",
    "y_train = [ ]<br>\n",
    "x_test = [ ]<br>\n",
    "y_test = [ ]<br>\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.9)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy  mothersday to all you amazing mothers o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy mothers day mum   i'm sorry i can't be t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy mothers day to all this doing a mothers ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy mothers day to this beautiful woman   ro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  sentiment_class\n",
       "0  happy  mothersday to all you amazing mothers o...                0\n",
       "1  happy mothers day mum   i'm sorry i can't be t...                0\n",
       "2  happy mothers day to all this doing a mothers ...               -1\n",
       "3  happy mothers day to this beautiful woman   ro...                0\n",
       "4  remembering the 3 most amazing ladies who made...               -1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardizing text in the tweet, removing all types of sybmbols from the tweets\n",
    "\n",
    "def standardize_tweets(tweet, text_field):\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'http','')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'!','')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'@S\\+','')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]',' ')\n",
    "    tweet[text_field] = tweet[text_field].str.replace(r'@','at')\n",
    "    tweet[text_field] = tweet[text_field].str.lower()\n",
    "    return tweet\n",
    "   \n",
    "    \n",
    "train_data = standardize_tweets(train_data, \"original_text\")\n",
    "train_data.to_csv(\"train_tweet.csv\")\n",
    "\n",
    "test_data = standardize_tweets(test_data, \"original_text\")\n",
    "test_data.to_csv(\"test_tweet.csv\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(train_data)\n",
    "X = cv.transform(pd.read_csv(\"./train_tweet.csv\"))\n",
    "Y = cv.transform(pd.read_csv(\"./test_tweet.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 original_text\n",
       "sentiment_class               \n",
       "-1                         769\n",
       " 0                        1701\n",
       " 1                         765"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('sentiment_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80874\n"
     ]
    }
   ],
   "source": [
    "def create_lexicon():\n",
    "    file = pd.read_csv('train_tweet.csv')\n",
    "    tweets = file['original_text']\n",
    "    lexicon = []\n",
    "    for line in tweets:\n",
    "        words = word_tokenize(line)\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        lexicon += words\n",
    "        \n",
    "    lexicon_without_stop_words = []\n",
    "    \n",
    "    for word in lexicon:\n",
    "        if word in stop_words:\n",
    "            continue;\n",
    "        else:\n",
    "            lexicon_without_stop_words.append(word)\n",
    "            \n",
    "    print(len(lexicon_without_stop_words))\n",
    "    \n",
    "    final_lexicon = []\n",
    "    \n",
    "    words = nltk.FreqDist(lexicon_without_stop_words)\n",
    "    \n",
    "    for word in words:\n",
    "        if words[word]>2 and len(word)>2:\n",
    "            final_lexicon.append(word)\n",
    "            \n",
    "    return final_lexicon\n",
    "\n",
    "lexicon = create_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2043\n"
     ]
    }
   ],
   "source": [
    "len_of_lexicon = len(lexicon)\n",
    "print(len_of_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAJNCAYAAACfsmlCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfbRlZX0n+O9PEN9jiZaMKdBSw5hkuiOaWgY1y1ZIOr5FnAQmZplIHGZId5ioSXrS6DLtOCbdmu6oMWtiN0uMZbftG1EhQtvSiKZ7eiSC74IOSBCqQSgj4Ft8QX/zx9k13pS3qo5a+z7Xez6ftc46ez/7Ofv8Hureu77s/ey9q7sDAMA4dxpdAADAqhPIAAAGE8gAAAYTyAAABhPIAAAGE8gAAAY7cnQB34/73e9+vXPnztFlAAAc0hVXXPG57t6+3rYf6EC2c+fOXH755aPLAAA4pKr6zIG2OWUJADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAw2KyBrKp+q6o+UVUfr6o3VtVdq+rBVXVZVV1dVW+uqqOmvneZ1q+Ztu+cszYAgM1itkBWVTuSPCfJru7+e0mOSPKMJC9L8oruPj7JrUnOmD5yRpJbu/tHkrxi6gcAsOXNfcryyCR3q6ojk9w9yU1JTkpy3rR9d5KnT8unTOuZtp9cVTVzfQAAwx051467+79V1b9Kcn2Sv03y7iRXJLmtu++Yuu1JsmNa3pHkhumzd1TV7Unum+Rzc9UIHD47z75wdAmHzXUvfcroEoAVM+cpy/tkcdTrwUl+OMk9kjxpna697yMH2bZ2v2dW1eVVdfnevXsPV7kAAMPMecryZ5L8dXfv7e5vJHlbksck2TadwkySY5PcOC3vSXJckkzb753k8/vvtLvP6e5d3b1r+/btM5YPALAx5gxk1yc5saruPs0FOznJlUkuTXLq1Of0JOdPyxdM65m2v6e7v+MIGQDAVjNbIOvuy7KYnP/BJB+bvuucJP80yW9X1TVZzBE7d/rIuUnuO7X/dpKz56oNAGAzmW1Sf5J094uSvGi/5muTPGqdvl9Nctqc9QAAbEbu1A8AMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADDYkaMLgFW38+wLR5cAwGCOkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADDZbIKuqh1XVh9e8vlBVz6uqo6vq4qq6enq/z9S/qupVVXVNVX20qh45V20AAJvJbIGsuz/V3Sd09wlJfjLJV5K8PcnZSS7p7uOTXDKtJ8mTkhw/vc5M8uq5agMA2Ew26pTlyUk+3d2fSXJKkt1T++4kT5+WT0ny+l54f5JtVfWADaoPAGCYjQpkz0jyxmn5mO6+KUmm9/tP7TuS3LDmM3umNgCALW32QFZVRyV5WpK3HqrrOm29zv7OrKrLq+ryvXv3Ho4SAQCG2ogjZE9K8sHuvnlav3nfqcjp/ZapfU+S49Z87tgkN+6/s+4+p7t3dfeu7du3z1g2AMDG2IhA9sv59unKJLkgyenT8ulJzl/T/qzpassTk9y+79QmAMBWduScO6+quyf52SS/vqb5pUneUlVnJLk+yWlT+0VJnpzkmiyuyHz2nLUBHMjOsy8cXcJhc91LnzK6BGAJsway7v5Kkvvu1/Y3WVx1uX/fTnLWnPUAAGxG7tQPADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADDYrIGsqrZV1XlV9cmquqqqHl1VR1fVxVV19fR+n6lvVdWrquqaqvpoVT1yztoAADaLuY+Q/XGSd3X3jyZ5eJKrkpyd5JLuPj7JJdN6kjwpyfHT68wkr565NgCATWG2QFZVP5TkcUnOTZLu/np335bklCS7p267kzx9Wj4lyet74f1JtlXVA+aqDwBgs5jzCNlDkuxN8mdV9aGqek1V3SPJMd19U5JM7/ef+u9IcsOaz++Z2gAAtrQ5A9mRSR6Z5NXd/YgkX863T0+up9Zp6+/oVHVmVV1eVZfv3bv38FQKADDQnIFsT5I93X3ZtH5eFgHt5n2nIqf3W9b0P27N549NcuP+O+3uc7p7V3fv2r59+2zFAwBslNkCWXd/NskNVfWwqenkJFcmuSDJ6VPb6UnOn5YvSPKs6WrLE5Pcvu/UJgDAVnbkzPv/zSRvqKqjklyb5NlZhMC3VNUZSa5PctrU96IkT05yTZKvTH0BALa8WQNZd384ya51Np28Tt9Octac9QAAbEbu1A8AMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMNiRowsAYD47z75wdAmHzXUvfcroEmA2jpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMJpABAAwmkAEADCaQAQAMNmsgq6rrqupjVfXhqrp8aju6qi6uqqun9/tM7VVVr6qqa6rqo1X1yDlrAwDYLDbiCNkTuvuE7t41rZ+d5JLuPj7JJdN6kjwpyfHT68wkr96A2gAAhjtkIKuqe1TVnabl/76qnlZVd/4+vvOUJLun5d1Jnr6m/fW98P4k26rqAd/H9wAA/EBY5gjZXya5a1XtyOKI1rOTvG7J/XeSd1fVFVV15tR2THfflCTT+/2n9h1Jbljz2T1TGwDAlnbkEn2qu79SVWck+ZPu/sOq+tCS+39sd99YVfdPcnFVffJg37NOW39Hp0WwOzNJHvjABy5ZBgDA5rXMEbKqqkcneWaSC6e2ZYJcuvvG6f2WJG9P8qgkN+87FTm93zJ135PkuDUfPzbJjevs85zu3tXdu7Zv375MGQAAm9oygex5SZ6f5O3d/YmqekiSSw/1oWnu2b32LSf5h0k+nuSCJKdP3U5Pcv60fEGSZ01XW56Y5PZ9pzYBALayQx7p6u73JXnfFKrS3dcmec4S+z4mydurat/3/PvufldVfSDJW6ZToNcnOW3qf1GSJye5JslXspirBgCw5R0ykE2nK89Ncs8kD6yqhyf59e7+jYN9bgpuD1+n/W+SnLxOeyc5a8m6AQC2jGVOWb4yyc8l+Zsk6e6PJHncnEUBAKySpW4M29037Nf0zRlqAQBYSctcLXlDVT0mSVfVUVnMH7tq3rIAAFbHMkfI/lEWc7t2ZHFrihNirhcAwGGzzFWWn8viHmQAAMxgmWdZ7q6qbWvW71NVr523LACA1bHMKcuf6O7b9q10961JHjFfSQAAq2WZQHanqrrPvpWqOjpLPjoJAIBDWyZY/VGS/1pV503rpyX5g/lKAgBYLctM6n99VV2R5AlJKskvdPeVs1cGALAilj31+Mkkt+7rX1UP7O7rZ6sKAGCFLPMsy99M8qIkN2dxh/5K0kl+Yt7SAABWwzJHyJ6b5GHTQ8EBADjMlrnK8oYkt89dCADAqlrmCNm1Sd5bVRcm+dq+xu5++WxVAQCskGUC2fXT66jpBQDAYbTMbS9enCRVdY/u/vL8JQEArJZlnmX56Kq6MslV0/rDq+pPZ68MAGBFLDOp/5VJfi7J3yRJd38kyePmLAoAYJUsE8jS3Tfs1/TNGWoBAFhJy0zqv6GqHpOkq+qoJM/JdPoSAIDv3zJHyP5RkrOS7EiyJ8kJSX5jzqIAAFbJMkfIHtbdz1zbUFWPTfJ/z1MSAMBqWeYI2Z8s2QYAwPfggEfIqurRSR6TZHtV/faaTT+U5Ii5CwMAWBUHO2V5VJJ7Tn3utab9C0lOnbMoAIBVcsBA1t3vS/K+qnpdd39mA2sCAFgpy0zqv0tVnZNk59r+3X3SXEUBAKySZQLZW5P86ySviRvCAgAcdssEsju6+9WzVwIAsKKWue3FX1TVb1TVA6rq6H2v2SsDAFgRyxwhO316/9/XtHWShxz+cgAAVs8hA1l3P3gjCgEAWFWHPGVZVXevqhdOV1qmqo6vqqfOXxoAwGpYZg7ZnyX5ehZ37U8WDxj//dkqAgBYMcsEsod29x8m+UaSdPffJqlZqwIAWCHLBLKvV9XdspjIn6p6aJKvzVoVAMAKWeYqyxcleVeS46rqDUkem+TX5iwKAGCVLHOV5cVV9cEkJ2ZxqvK53f252SsDAFgRy1xl+dgkX+3uC5NsS/KCqnrQ7JUBAKyIZeaQvTrJV6rq4VncHPYzSV4/a1UAACtkmUB2R3d3klOSvKq7/zjJveYtCwBgdSwzqf+LVfX8JL+S5HFVdUSSO89bFgDA6ljmCNkvZXGbizO6+7NJdiT5l7NWBQCwQpa5yvKzSV6+Zv36mEMGAHDYLHOEDACAGQlkAACDHTCQVdUl0/vLNq4cAIDVc7A5ZA+oqn+Q5GlV9abs90Dx7v7grJUBAKyIgwWyf5bk7CTHZs2k/kknOWmuogAAVskBA1l3n5fkvKr6ve5+yQbWBACwUpa57cVLquppSR43Nb23u985b1kAAKtjmYeL/4skz01y5fR67tQGAMBhsMyjk56S5ITu/laSVNXuJB9K8vw5CwMAWBXL3ods25rle89RCADAqlrmCNm/SPKhqro0i1tfPC6OjgEAHDaHPELW3W9McmKSt02vR3f3m5b9gqo6oqo+VFXvnNYfXFWXVdXVVfXmqjpqar/LtH7NtH3n9zIgAIAfNEudsuzum7r7gu4+f3rY+HfjuUmuWrP+siSv6O7jk9ya5Iyp/Ywkt3b3jyR5xdQPAGDLm/VZllV1bBYXBbxmWq8sbih73tRld5KnT8unTOuZtp889QcA2NLmfrj4K5P8bpJvTev3TXJbd98xre9JsmNa3pHkhiSZtt8+9QcA2NIOGsiq6k5V9fHvZcdV9dQkt3T3FWub1+naS2xbu98zq+ryqrp8796930tpAACbykED2XTvsY9U1QO/h30/NosHk1+X5E1ZnKp8ZZJtVbXv6s5jk9w4Le9JclySTNvvneTz69R0Tnfv6u5d27dv/x7KAgDYXJY5ZfmAJJ+oqkuq6oJ9r0N9qLuf393HdvfOJM9I8p7ufmaSS5OcOnU7Pcn50/IF03qm7e/p7u84QgYAsNUscx+yFx/m7/ynSd5UVb+fxR3/z53az03yb6vqmiyOjD3jMH8vAMCmtMzDxd9XVQ9Kcnx3/6equnuSI76bL+nu9yZ577R8bZJHrdPnq0lO+272CwCwFSzzcPH/NYvbUPybqWlHknfMWRQAwCpZZg7ZWVlM0P9CknT31UnuP2dRAACrZJlA9rXu/vq+lekKSJPtAQAOk2UC2fuq6gVJ7lZVP5vkrUn+Yt6yAABWxzKB7Owke5N8LMmvJ7koyQvnLAoAYJUsc5Xlt6pqd5LLsjhV+Sn3BwMAOHwOGciq6ilJ/nWST2fxeKMHV9Wvd/d/mLs4AIBVsMyNYf8oyRO6+5okqaqHJrkwiUAGAHAYLDOH7JZ9YWxybZJbZqoHAGDlHPAIWVX9wrT4iaq6KMlbsphDdlqSD2xAbQAAK+Fgpyx/fs3yzUn+wbS8N8l9ZqsIAGDFHDCQdfezN7IQAIBVtcxVlg9O8ptJdq7t391Pm68sAIDVscxVlu9Icm4Wd+f/1rzlAACsnmUC2Ve7+1WzVwIAsKKWCWR/XFUvSvLuJF/b19jdH5ytKgCAFbJMIPv7SX41yUn59inLntYBAPg+LRPI/sckD+nur89dDADAKlrmTv0fSbJt7kIAAFbVMkfIjknyyar6QP7uHDK3vQAAOAyWCWQvmr0KAIAVdshA1t3v24hCAABW1TJ36v9iFldVJslRSe6c5Mvd/UNzFgYAsCqWOUJ2r7XrVfX0JI+arSIAgBWzzFWWf0d3vyPuQQYAcNgsc8ryF9as3inJrnz7FCYAAN+nZa6y/Pk1y3ckuS7JKbNUAwCwgpaZQ/bsjSgEAGBVHTCQVdU/O8jnurtfMkM9AAAr52BHyL68Tts9kpyR5L5JBDIAgMPggIGsu/9o33JV3SvJc5M8O8mbkvzRgT4HAMB356BzyKrq6CS/neSZSXYneWR337oRhQEArIqDzSH7l0l+Ick5Sf5+d39pw6oCAFghB7sx7O8k+eEkL0xyY1V9YXp9saq+sDHlAQBsfQebQ/Zd38UfAIDvntAFADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMNhsgayq7lpVf1VVH6mqT1TVi6f2B1fVZVV1dVW9uaqOmtrvMq1fM23fOVdtAACbyZxHyL6W5KTufniSE5I8sapOTPKyJK/o7uOT3JrkjKn/GUlu7e4fSfKKqR8AwJY3WyDrhS9Nq3eeXp3kpCTnTe27kzx9Wj5lWs+0/eSqqrnqAwDYLGadQ1ZVR1TVh5PckuTiJJ9Oclt33zF12ZNkx7S8I8kNSTJtvz3JfeesDwBgM5g1kHX3N7v7hCTHJnlUkh9br9v0vt7RsN6/oarOrKrLq+ryvXv3Hr5iAQAG2ZCrLLv7tiTvTXJikm1VdeS06dgkN07Le5IclyTT9nsn+fw6+zqnu3d1967t27fPXToAwOzmvMpye1Vtm5bvluRnklyV5NIkp07dTk9y/rR8wbSeaft7uvs7jpABAGw1Rx66y/fsAUl2V9URWQS/t3T3O6vqyiRvqqrfT/KhJOdO/c9N8m+r6posjow9Y8baAAA2jdkCWXd/NMkj1mm/Nov5ZPu3fzXJaXPVAwCwWblTPwDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgAhkAwGACGQDAYAIZAMBgR44uAL4XO8++cHQJAHDYOEIGADCYQAYAMJhABgAwmEAGADCYSf0A/EDYKhfzXPfSp4wugU1otiNkVXVcVV1aVVdV1Seq6rlT+9FVdXFVXT2932dqr6p6VVVdU1UfrapHzlUbAMBmMucpyzuS/E53/1iSE5OcVVU/nuTsJJd09/FJLpnWk+RJSY6fXmcmefWMtQEAbBqzBbLuvqm7PzgtfzHJVUl2JDklye6p2+4kT5+WT0ny+l54f5JtVfWAueoDANgsNmRSf1XtTPKIJJclOaa7b0oWoS3J/aduO5LcsOZje6Y2AIAtbfZAVlX3TPLnSZ7X3V84WNd12nqd/Z1ZVZdX1eV79+49XGUCAAwzayCrqjtnEcbe0N1vm5pv3ncqcnq/ZWrfk+S4NR8/NsmN+++zu8/p7l3dvWv79u3zFQ8AsEHmvMqykpyb5KrufvmaTRckOX1aPj3J+WvanzVdbXliktv3ndoEANjK5rwP2WOT/GqSj1XVh6e2FyR5aZK3VNUZSa5Pctq07aIkT05yTZKvJHn2jLUBAGwaswWy7v4vWX9eWJKcvE7/TnLWXPUAAGxWHp0EADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADCYQAYAMJhABgAwmEAGADDYbIGsql5bVbdU1cfXtB1dVRdX1dXT+32m9qqqV1XVNVX10ap65Fx1AQBsNnMeIXtdkifu13Z2kku6+/gkl0zrSfKkJMdPrzOTvHrGugAANpXZAll3/2WSz+/XfEqS3dPy7iRPX9P++l54f5JtVfWAuWoDANhMNnoO2THdfVOSTO/3n9p3JLlhTb89UxsAwJa3WSb11zptvW7HqjOr6vKqunzv3r0zlwUAML+NDmQ37zsVOb3fMrXvSXLcmn7HJrlxvR109zndvau7d23fvn3WYgEANsJGB7ILkpw+LZ+e5Pw17c+arrY8Mcnt+05tAgBsdUfOteOqemOSxye5X1XtSfKiJC9N8paqOiPJ9UlOm7pflOTJSa5J8pUkz56rLgCAzWa2QNbdv3yATSev07eTnDVXLQAAm9lmmdQPALCyBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwQQyAIDBBDIAgMEEMgCAwY4cXQAArJKdZ184uoTD5rqXPmV0CVuGQLZCttIfAQDYSpyyBAAYTCADABhMIAMAGGxTBbKqemJVfaqqrqmqs0fXAwCwETZNIKuqI5L8X0melOTHk/xyVf342KoAAOa3ma6yfFSSa7r72iSpqjclOSXJlSOLcmUiADC3zRTIdiS5Yc36niQ/NagWAOAQttJBi9H3VNtMgazWaevv6FR1ZpIzp9UvVdWnZq0quV+Sz838HZvZKo9/lceerPb4jX11rfL4V3nsqZdtyPgfdKANmymQ7Uly3Jr1Y5PcuH+n7j4nyTkbVVRVXd7duzbq+zabVR7/Ko89We3xG/tqjj1Z7fGv8tiT8ePfNJP6k3wgyfFV9eCqOirJM5JcMLgmAIDZbZojZN19R1X9b0n+Y5Ijkry2uz8xuCwAgNltmkCWJN19UZKLRtexnw07PbpJrfL4V3nsyWqP39hX1yqPf5XHngwef3V/x7x5AAA20GaaQwYAsJIEsoNYtUc5VdVrq+qWqvr4mrajq+riqrp6er/PyBrnUlXHVdWlVXVVVX2iqp47tW/58VfVXavqr6rqI9PYXzy1P7iqLpvG/ubpYpstqaqOqKoPVdU7p/VVGvt1VfWxqvpwVV0+tW35n/skqaptVXVeVX1y+t1/9AqN/WHTv/m+1xeq6nkrNP7fmv7efbyq3jj9HRz6ey+QHcCKPsrpdUmeuF/b2Uku6e7jk1wyrW9FdyT5ne7+sSQnJjlr+vdehfF/LclJ3f3wJCckeWJVnZjkZUleMY391iRnDKxxbs9NctWa9VUae5I8obtPWHPJ/yr83CfJHyd5V3f/aJKHZ/EzsBJj7+5PTf/mJyT5ySRfSfL2rMD4q2pHkuck2dXdfy+LCwmfkcG/9wLZgf3/j3Lq7q8n2fcopy2ru/8yyef3az4lye5peXeSp29oURuku2/q7g9Oy1/M4g/zjqzA+HvhS9PqnadXJzkpyXlT+5Yce5JU1bFJnpLkNdN6ZUXGfhBb/ue+qn4oyeOSnJsk3f317r4tKzD2dZyc5NPd/ZmszviPTHK3qjoyyd2T3JTBv/cC2YGt9yinHYNqGemY7r4pWYSWJPcfXM/sqmpnkkckuSwrMv7plN2Hk9yS5OIkn05yW3ffMXXZyj//r0zyu0m+Na3fN6sz9mQRvt9dVVdMT0JJVuPn/iFJ9ib5s+l09Wuq6h5ZjbHv7xlJ3jgtb/nxd/d/S/KvklyfRRC7PckVGfx7L5Ad2FKPcmJrqap7JvnzJM/r7i+MrmejdPc3p1MXx2ZxdPjH1uu2sVXNr6qemuSW7r5ibfM6Xbfc2Nd4bHc/MovpGWdV1eNGF7RBjkzyyCSv7u5HJPlytuDpuUOZ5kk9LclbR9eyUaZ5cackeXCSH05yjyx+/ve3oe8+gQsAAAYnSURBVL/3AtmBLfUopxVwc1U9IEmm91sG1zObqrpzFmHsDd39tql5ZcafJNMpm/dmMY9u23Q4P9m6P/+PTfK0qroui2kJJ2VxxGwVxp4k6e4bp/dbsphD9Kisxs/9niR7uvuyaf28LALaKox9rScl+WB33zytr8L4fybJX3f33u7+RpK3JXlMBv/eC2QH5lFOCxckOX1aPj3J+QNrmc00b+jcJFd198vXbNry46+q7VW1bVq+WxZ/rK5KcmmSU6duW3Ls3f387j62u3dm8Tv+nu5+ZlZg7ElSVfeoqnvtW07yD5N8PCvwc9/dn01yQ1U9bGo6OcmVWYGx7+eX8+3TlclqjP/6JCdW1d2nv/37/u2H/t67MexBVNWTs/i/5X2PcvqDwSXNqqremOTxSe6X5OYkL0ryjiRvSfLALH6IT+vu/Sf+/8Crqp9O8p+TfCzfnkv0gizmkW3p8VfVT2QxgfWILP4n7S3d/X9W1UOyOGp0dJIPJfmV7v7auErnVVWPT/JPuvupqzL2aZxvn1aPTPLvu/sPquq+2eI/90lSVSdkcTHHUUmuTfLsTL8D2eJjT5KqunsWc6Uf0t23T22r8m//4iS/lMUV9h9K8r9kMWds2O+9QAYAMJhTlgAAgwlkAACDCWQAAIMJZAAAgwlkAACDCWTAYVNVXzp0r+9r/8+bLtX/vr+vqu5SVf+pqj5cVb90eCr8ju94wRz7BbYegQz4QfK8LB4EfDg8Ismdu/uE7n7zYdrn/gQyYCkCGTCrqnpoVb1renj1f66qH53aX1dVr6qq/1pV11bVqVP7narqT6vqE1X1zqq6qKpOrarnZPHcuUur6tI1+/+DqvpIVb2/qo5Z5/uPrqp3VNVHpz4/UVX3T/LvkpwwHSF76H6feU5VXTl95k1T2z2q6rVV9YHpYdSnTO2/VlVvm8Z4dVX94dT+0iR3m/b/hqntV6rqr6a2f1NVR0ztX1pvHFV1TFW9fWr/SFU95kD7mV6vq6qPV9XHquq3Dus/JDCv7vby8vI6LK8kX1qn7ZIkx0/LP5XF44mS5HVZPND4Tkl+PMk1U/upSS6a2v+7JLcmOXXadl2S+63Zdyf5+Wn5D5O8cJ3v/5MkL5qWT0ry4Wn58UneeYBx3JjkLtPytun9n2dx5+4k2Zbk/83iocS/lsVd3u+d5K5JPpPkuP3/e2TxwPa/yOKoXJL8aZJnHWwcSd6cxYPuk8WTFO59oP0k+ckkF6/5vm2jfx68vLyWf+17iCbAYVdV98ziob1vXTwyLklylzVd3tHd30py5ZqjWz+d5K1T+2fXHg1bx9eTvHNaviLJz67T56eT/GKSdPd7quq+VXXvQ5T+0SRvqKp3ZPH4sGTxnMenVdU/mdbvmsXjZZLkkv72o2euTPKgLB5Js9bJWYSmD0z/Le6Wbz+4+UDjOCmLsJXu/maS26vqVw+wn79I8pCq+pMkFyZ59yHGCGwiAhkwpzslua27TzjA9rXPiav93pfxje7e9/y3b2b9v2nr7e9Qz4x7SpLHJXlakt+rqv9h2s8vdven/s7Oq34qf3ccB6tjd3c/f51ty4zjkPupqocn+bkkZyX5n5L8zwfZD7CJmEMGzKa7v5Dkr6vqtCSphYcf4mP/JckvTnPJjsni1OI+X0xyr++yjL9M8szp+x+f5HNTXeuqqjtlccrx0iS/m8XpyXsm+Y9JfrOmw1JV9YglvvsbVXXnafmSJKdO89f2zW170CE+f0mSfzz1P6KqfuhA+6mq+yW5U3f/eZLfS/LIJeoDNglHyIDD6e5VtWfN+suzCEOvrqoXJrlzkjcl+chB9vHnWZze+3gW87QuS3L7tO2cJP+hqm7q7icsWdP/keTPquqjSb6S5PRD9D8iyb+bTmtWkld0921V9ZIkr0zy0SmUXZfkqYfY1zlT/w929zOn/wbvnkLfN7I4kvWZg3z+uUnOqaozsjhy9o+7+/85wH7+dhrnvv/RXu9IHLBJ1bePkgNsDlV1z+7+UlXdN8lfJXlsd392dF0Ac3GEDNiM3llV25IcleQlwhiw1TlCBgAwmEn9AACDCWQAAIMJZAAAgwlkAACDCWQAAIMJZAAAg/1/CygqR/pa0GsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of a sentence:  38.76259659969088\n"
     ]
    }
   ],
   "source": [
    "def plot_length_sentences():\n",
    "    file = pd.read_csv('train_tweet.csv')\n",
    "    tweets = file['original_text']\n",
    "    sentences_length = []\n",
    "    for line in tweets:\n",
    "        words = word_tokenize(line)\n",
    "        sentences_length.append(len(words))\n",
    "        \n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    plt.xlabel('Length of sentences')\n",
    "    plt.ylabel('Number of sentences')\n",
    "    plt.hist(sentences_length, range=(0, 80))\n",
    "    plt.show()\n",
    "    \n",
    "    return sentences_length\n",
    "\n",
    "sentences_length = plot_length_sentences()\n",
    "print('Average length of a sentence: ',sum(sentences_length)/len(train_data['original_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360000/360000 [07:00<00:00, 855.42it/s]\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = []\n",
    "word_embeddings_word = []\n",
    "i=0\n",
    "\n",
    "with open('../glove.6B.100d.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    len_ = len(lines)\n",
    "    for line in tqdm(lines[:int(.9*len_)]):\n",
    "        line = line.lower()\n",
    "        words = word_tokenize(line)\n",
    "        word_embeddings_word.append(words[0])\n",
    "        word_embeddings.append(words[1:])\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 34/2043 [00:00<00:05, 339.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2043/2043 [00:07<00:00, 257.09it/s]\n"
     ]
    }
   ],
   "source": [
    "print(len(word_embeddings_word))\n",
    "index_of_a_word_in_lexicon_in_word_embeddings = []\n",
    "value = []\n",
    "for word in tqdm(lexicon):\n",
    "    if word in word_embeddings_word:\n",
    "        value.append(word)\n",
    "        index_in_word_embeddings = word_embeddings_word.index(word)\n",
    "        index_of_a_word_in_lexicon_in_word_embeddings.append(index_in_word_embeddings)\n",
    "    else:\n",
    "        index_of_a_word_in_lexicon_in_word_embeddings.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clean_tweets_train = pd.read_csv('train_tweet.csv')\n",
    "outputs=[]\n",
    "\n",
    "list_corpus = train_data['original_text']\n",
    "list_labels = train_data['sentiment_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels,test_size=0.1,random_state=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2911\n"
     ]
    }
   ],
   "source": [
    "y_train_dual = []\n",
    "y_test_dual = []\n",
    "\n",
    "for i in y_train:\n",
    "    if i == 1:\n",
    "        y_train_dual.append([1, 0])\n",
    "    elif i == 0:\n",
    "        y_train_dual.append([0, 1])\n",
    "    else:\n",
    "        y_train_dual.append([0, 0])\n",
    "    \n",
    "for i in y_test:\n",
    "    if i == 1:\n",
    "        y_test_dual.append([1, 0])\n",
    "    elif i == 0:\n",
    "        y_test_dual.append([0, 1])\n",
    "    else:\n",
    "        y_test_dual.append([0, 0])\n",
    "    \n",
    "y_train = y_train_dual\n",
    "y_test = y_test_dual\n",
    "\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_process(lines):\n",
    "    lengths = [len(line) for line in lines]\n",
    "    max_len = max(lengths)\n",
    "    padded_lines = pad_sequences(lines, maxlen=max_len,dtype='str', padding='post',truncating='post')\n",
    "    \n",
    "    return padded_lines, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(lines):\n",
    "    list_of_eg = []\n",
    "    for line in tqdm(lines):\n",
    "        line = str(lines)\n",
    "        line = line.lower()\n",
    "        word = word_tokenize(line)\n",
    "        \n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        list_of_eg.append(lemmatized_words)\n",
    "    return list_of_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 298.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 101, 93)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_vector(lines, max_len):\n",
    "    no_of_samples = len(lines)\n",
    "    features_of_batch = []\n",
    "    for line in lines:\n",
    "        features_of_line = []\n",
    "        for word in line:\n",
    "            feature = np.zeros(93)\n",
    "            if word in value:\n",
    "                indec_in_lexicon = lexicon.index(word)\n",
    "                feature = word_embeddings[index_of_a_word_in_lexicon_in_word_embeddings[index_in_lexicon]][3:96]\n",
    "            features_of_line.append(feature)\n",
    "        features_of_batch.append(list(features_of_line))\n",
    "    features_of_batch = np.array(features_of_batch)\n",
    "    features_of_batch = np.reshape(features_of_batch, (no_of_samples,max_len, 93))\n",
    "    print(features_of_batch.shape)\n",
    "    \n",
    "    return features_of_batch\n",
    "\n",
    "features = convert_to_list(X_train[:128])\n",
    "features, lengths = padding_process(features)\n",
    "features = convert_to_vector(features, max(lengths))\n",
    "\n",
    "print(features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "batch_size = 128\n",
    "rnn_size = 12\n",
    "len_of_word_embedding_vector = 93\n",
    "output_nodes = 2\n",
    "\n",
    "x = tf.placeholder('float',name='input_labels', shape=(None, None, 93))\n",
    "y = tf.placeholder('float', name='output_labels')\n",
    "dropout_prob = tf.placeholder('float', name='keep_prob')\n",
    "input_sequence_length = tf.placeholder(dtype=tf.int32, shape=[None],name='input_sequence_length')\n",
    "max_len = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "hm_epochs = 30\n",
    "output_layer = {'weight':tf.Variable(\n",
    "    tf.random_normal([rnn_size*2, output_nodes], name='weights')),'biases':tf.Variable(tf.random_normal\n",
    "                                                                            ([1, output_nodes], name='biases'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def neural_network_model(x, input_sequence_length, max_len, keep_prob):\n",
    "    with tf.name_scope('neural_network'):\n",
    "        forward_cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "        backward_cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "        \n",
    "        forward_drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(forward_cell,input_keep_prob=keep_prob)\n",
    "        backward_drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(backward_cell, input_keep_prob=keep_prob)\n",
    "        \n",
    "        x = tf.transpose(x, [1,0,2])\n",
    "        bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(forward_drop, backward_drop, x, \n",
    "                                        dtype=tf.float32, sequence_length=input_sequence_length,time_major=True)\n",
    "        output = tf.concat(encoder_state, -1)\n",
    "        \n",
    "        outputs = tf.linalg.matmul(output[0], output_layer['weight']) + output_layer['biases']\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"neural_network/bidirectional_rnn/fw/fw/Const:0\", shape=(1,), dtype=int32) must be from the same graph as Tensor(\"Equal_2:0\", shape=(1,), dtype=bool).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-26ccbdb5c9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                          lengths, max_len:max(lengths)}))\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-26ccbdb5c9e3>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-ca32151fa3f2>\u001b[0m in \u001b[0;36mneural_network_model\u001b[0;34m(x, input_sequence_length, max_len, keep_prob)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(forward_drop, backward_drop, x, \n\u001b[0;32m---> 11\u001b[0;31m                                         dtype=tf.float32, sequence_length=input_sequence_length,time_major=True)\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    462\u001b[0m           \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m           \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m           scope=fw_scope)\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;31m# Backward direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;31m# Perform some shape validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m       with ops.control_dependencies(\n\u001b[0;32m--> 694\u001b[0;31m           [_assert_has_shape(sequence_length, [batch_size])]):\n\u001b[0m\u001b[1;32m    695\u001b[0m         sequence_length = array_ops.identity(\n\u001b[1;32m    696\u001b[0m             sequence_length, name=\"CheckSeqLen\")\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py\u001b[0m in \u001b[0;36m_assert_has_shape\u001b[0;34m(x, shape)\u001b[0m\n\u001b[1;32m    684\u001b[0m       \u001b[0mpacked_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m       return control_flow_ops.Assert(\n\u001b[0;32m--> 686\u001b[0;31m           math_ops.reduce_all(math_ops.equal(x_shape, packed_shape)), [\n\u001b[0m\u001b[1;32m    687\u001b[0m               \u001b[0;34m\"Expected shape for Tensor %s is \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m               \u001b[0;34m\" but saw shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_all\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2353\u001b[0m       gen_math_ops._all(\n\u001b[1;32m   2354\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m           name=name))\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_all\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m    550\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m    551\u001b[0m         \u001b[0;34m\"All\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                name=name)\n\u001b[0m\u001b[1;32m    553\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5881\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5882\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5883\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5884\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5885\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5816\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5817\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[0;32m-> 5818\u001b[0;31m                      (item, original_item))\n\u001b[0m\u001b[1;32m   5819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"neural_network/bidirectional_rnn/fw/fw/Const:0\", shape=(1,), dtype=int32) must be from the same graph as Tensor(\"Equal_2:0\", shape=(1,), dtype=bool)."
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x, input_sequence_length, max_len, dropout_prob)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "        summary = tf.summary.scalar('t', cost)\n",
    "    \n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter('./gaurav', sess.graph)\n",
    "        k=0\n",
    "        len_of_train_data = len(X_train)\n",
    "        for epochs in tqdm(range(hm_epochs)):\n",
    "            epoch_loss =0\n",
    "            starting = 0\n",
    "            end = batch_size\n",
    "            \n",
    "            while end<=len_of_train_data:\n",
    "                epoch_x = X_train[starting:end]\n",
    "                epoch_y = y_train[starting:end]\n",
    "                starting = end\n",
    "                end = end+batch_size\n",
    "                epoch_x = convert_to_list(epoch_x)\n",
    "                new_epoch_x = []\n",
    "                new_epoch_y = []\n",
    "                i=0\n",
    "                \n",
    "                for line in tqdm(eopch_x):\n",
    "                    if len(line) < 20:\n",
    "                        new_epoch_x.append(line)\n",
    "                        new_epoch_x.append(epoch_y[i])\n",
    "                    i = i+1\n",
    "                epoch_x, lengths = padding_process(new_epoch_x)\n",
    "                epoch_x = convert_to_vector(epoch_x, max(lengths))\n",
    "                epoch_y = new_epoch_y\n",
    "                _, c, m=sess.run([optimizer, cost, merged], feed_dict={x: epoch_x, y: epoch_y, max_len: max(lengths),\n",
    "                                    input_sequence_length:lengths, dropout_prob: 0.7, learning: 8.0/(1000+80*epoch)})\n",
    "                epoch_loss += c\n",
    "                train_writer.add_summary(m, k)\n",
    "                k += 1\n",
    "            print('loss after ', epoch, 'is', epoch_loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "            epoch_y = y_test\n",
    "            epoch_x = X_test\n",
    "            epoch_x = convert_to_list(epoch_x)\n",
    "            new_epoch_x = []\n",
    "            new_epoch_y = []\n",
    "            i = 0\n",
    "            for line in tqdm(epoch_x):\n",
    "                if len(line)<20:\n",
    "                    new_epoch_x.append(line)\n",
    "                    new_epoch_y.append(epoch_y[i])\n",
    "                i = i+1\n",
    "            epoch_x, lengths= padding_process(new_epoch_x)\n",
    "            epoch_x = convert_to_vector(epoch_x, max(lengths))\n",
    "            epoch_y = new_epoch_y\n",
    "\n",
    "        print('Accuracy:', accuracy.eval({x:epoch_x, y:epoch_y, dropout_prob: 1.0, input_sequence_length\n",
    "                                         :lengths, max_len:max(lengths)}))\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        epoch_y = y_test\n",
    "        epoch_x = X_test\n",
    "        epoch_x = convert_to_list(epoch_x)\n",
    "        new_epoch_x = []\n",
    "        new_epoch_y = []\n",
    "        i=0\n",
    "        for line in tqdm(epoch_x):\n",
    "            if len(line) < 10:\n",
    "                new_epoch_x.append(line)\n",
    "                new_epoch_y.append(epoch_y[i])\n",
    "            i+=1\n",
    "        epoch_x, lengths= padding_process(new_epoch_x)\n",
    "        epoch_x = convert_to_vector(epoch_x, max(lengths))\n",
    "\n",
    "        epoch_y = new_epoch_y\n",
    "\n",
    "        print('Accuracy:', accuracy.eval({x:epoch_x, y:epoch_y, dropout_prob:1.0, input_sequence_length:\n",
    "                                         lengths, max_len:max(lengths)}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 164,106\n",
      "Trainable params: 164,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
